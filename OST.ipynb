{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FILENAME    0\n",
       "IDENTITY    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"written_name_train_v2.csv\")\n",
    "train_data.dropna(inplace=True)\n",
    "train_data.drop(train_data[train_data['IDENTITY'].str.lower()=='unreadable'].index, inplace=True)\n",
    "train_data.reset_index(drop=True, inplace=True)\n",
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FILENAME    0\n",
       "IDENTITY    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(\"written_name_test_v2.csv\")\n",
    "test.dropna(inplace=True)\n",
    "test.drop(test[test['IDENTITY'].str.lower()=='unreadable'].index, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FILENAME    0\n",
       "IDENTITY    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation = pd.read_csv(\"written_name_validation_v2.csv\")\n",
    "validation.dropna(inplace=True)\n",
    "validation.drop(validation[validation['IDENTITY'].str.lower()=='unreadable'].index, inplace=True)\n",
    "validation.reset_index(drop=True, inplace=True)\n",
    "validation.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['IDENTITY'] = train_data['IDENTITY'].str.upper()\n",
    "validation['IDENTITY'] = validation['IDENTITY'].str.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.io import imread_collection as imd\n",
    "import imageio as im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size=10000\n",
    "valid_size=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    (h, w) = img.shape\n",
    "    \n",
    "    final_img = np.ones([64, 256])*255 # blank white image\n",
    "    \n",
    "    # crop\n",
    "    if w > 256:\n",
    "        img = img[:, :256]\n",
    "        \n",
    "    if h > 64:\n",
    "        img = img[:64, :]\n",
    "    \n",
    "    \n",
    "    final_img[:h, :w] = img\n",
    "    final_img=final_img.swapaxes(-2,-1)[...,::-1]\n",
    "    return final_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n"
     ]
    }
   ],
   "source": [
    "train_img=[]\n",
    "im_main=\"train_v2/train/\"\n",
    "for i in range(train_size):\n",
    "    im_dir=im_main+train_data.loc[i,'FILENAME']\n",
    "    img=im.imread(im_dir)\n",
    "    img=rgb2gray(img)\n",
    "    img=preprocess(img)\n",
    "    img=img/255\n",
    "    train_img.append(img)\n",
    "    if i%1000==0:\n",
    "        print(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "valid_img=[]\n",
    "im_main=\"validation_v2/validation/\"\n",
    "for i in range(valid_size):\n",
    "    im_dir=im_main+validation.loc[i,'FILENAME']\n",
    "    img=im.imread(im_dir)\n",
    "    img=rgb2gray(img)\n",
    "    img=preprocess(img)\n",
    "    img=img/255\n",
    "    valid_img.append(img)\n",
    "    if i%500==0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img = np.array(train_img).reshape(-1, 256, 64, 1)\n",
    "valid_img = np.array(valid_img).reshape(-1, 256, 64, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 256, 64, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 256, 64, 1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensonality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(.9995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=0.9995, random_state=None,\n",
       "    svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.fit(train_img[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(500):\n",
    "    pca.fit(train_img[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_img)):\n",
    "    x=pca.transform(train_img[i])\n",
    "    train_img[i]=x\n",
    "    if i%500==0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(valid_img)):\n",
    "    valid_img[i]=pca.transform(valid_img[i])\n",
    "    if i%500==0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20809, 256, 64, 1)"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\sourabh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\sourabh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\sourabh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\sourabh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\sourabh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\sourabh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\sourabh\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\sourabh\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\sourabh\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\sourabh\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\sourabh\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\sourabh\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabets = u\"ABCDEFGHIJKLMNOPQRSTUVWXYZ-' \"\n",
    "max_str_len = 24 \n",
    "num_of_characters = len(alphabets) + 1 \n",
    "num_of_timestamps = 32 \n",
    "\n",
    "\n",
    "def label_to_num(label):\n",
    "    label_num = []\n",
    "    for ch in label:\n",
    "        label_num.append(alphabets.find(ch))\n",
    "        \n",
    "    return np.array(label_num)\n",
    "\n",
    "def num_to_label(num):\n",
    "    ret = \"\"\n",
    "    for ch in num:\n",
    "        if ch == -1:  \n",
    "            break\n",
    "        else:\n",
    "            ret+=alphabets[ch]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.ones([train_size, max_str_len]) * -1\n",
    "train_label_len = np.zeros([train_size, 1])\n",
    "train_input_len = np.ones([train_size, 1]) * (num_of_timestamps-2)\n",
    "train_output = np.zeros([train_size])\n",
    "\n",
    "for i in range(train_size):\n",
    "    train_label_len[i] = len(train_data.loc[i, 'IDENTITY'])\n",
    "    train_y[i, 0:len(train_data.loc[i, 'IDENTITY'])]= label_to_num(train_data.loc[i, 'IDENTITY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_y = np.ones([valid_size, max_str_len]) * -1\n",
    "valid_label_len = np.zeros([valid_size, 1])\n",
    "valid_input_len = np.ones([valid_size, 1]) * (num_of_timestamps-2)\n",
    "valid_output = np.zeros([valid_size])\n",
    "\n",
    "for i in range(valid_size):\n",
    "    valid_label_len[i] = len(validation.loc[i, 'IDENTITY'])\n",
    "    valid_y[i, 0:len(validation.loc[i, 'IDENTITY'])]= label_to_num(validation.loc[i, 'IDENTITY']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_func(args):\n",
    "    ypred, labels, input_length, label_length=args\n",
    "    ypred = ypred[:, 2:, :]\n",
    "    return K.ctc_batch_cost(labels, ypred, input_length, label_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels = Input(name='gtruth_labels', shape=[max_str_len], dtype='float32')\n",
    "input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
    "label_length = Input(name='label_length', shape=[1], dtype='int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Reshape, Bidirectional, LSTM, Dense, Lambda, Activation, BatchNormalization, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sourabh\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "l0=Input(shape=(256, 64, 1))\n",
    "l1=Conv2D(32, (3, 3), padding='same',activation='relu')(l0)\n",
    "l2=BatchNormalization()(l1)\n",
    "l3=MaxPooling2D(pool_size=(2, 2))(l2)\n",
    "l4=Conv2D(64, (3, 3), padding='same',activation='relu')(l3)\n",
    "l5=BatchNormalization()(l4)\n",
    "l6=MaxPooling2D(pool_size=(2, 2))(l5)\n",
    "l7=Dropout(0.2)(l6)\n",
    "l8=Conv2D(128, (3, 3), padding='same',activation='relu')(l7)\n",
    "l9=BatchNormalization()(l8)\n",
    "l10=MaxPooling2D(pool_size=(2, 2))(l9)\n",
    "l11=Dropout(0.2)(l10)\n",
    "l12=Reshape(target_shape=((32,1024)))(l11)\n",
    "l13=Dense(21, activation='relu')(l12)\n",
    "l14=Bidirectional(LSTM(256, return_sequences=True))(l13)\n",
    "l15=Bidirectional(LSTM(256, return_sequences=True))(l14)\n",
    "l16=Bidirectional(LSTM(256, return_sequences=True))(l15)\n",
    "ypred=Dense(num_of_characters,activation='softmax')(l16)\n",
    "model = Model(inputs=l0, outputs=ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 256, 64, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 256, 64, 32)       320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256, 64, 32)       128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 128, 32, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 128, 32, 64)       18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 128, 32, 64)       256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 64, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 64, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 64, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 32, 8, 128)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32, 8, 128)        0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 32, 1024)          0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32, 21)            21525     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 32, 512)           569344    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 32, 512)           1574912   \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 32, 512)           1574912   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32, 30)            15390     \n",
      "=================================================================\n",
      "Total params: 3,849,651\n",
      "Trainable params: 3,849,203\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sourabh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "ctc_loss = Lambda(ctc_func, output_shape=(1,), name='ctc')([ypred, labels, input_length, label_length])\n",
    "model = Model(inputs=[l0, labels, input_length, label_length], outputs=ctc_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=SGD(lr = 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sourabh\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/4\n",
      "10000/10000 [==============================] - 1189s 119ms/step - loss: 223.8932 - val_loss: 233.6609\n",
      "Epoch 2/4\n",
      "10000/10000 [==============================] - 1154s 115ms/step - loss: 232.8381 - val_loss: 233.6609\n",
      "Epoch 3/4\n",
      "10000/10000 [==============================] - 1161s 116ms/step - loss: 232.8381 - val_loss: 233.6609\n",
      "Epoch 4/4\n",
      "10000/10000 [==============================] - 1175s 118ms/step - loss: 232.8381 - val_loss: 233.6609\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1ca653e0cc8>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[train_img, train_y, train_input_len, train_label_len], y=train_output, \n",
    "                validation_data=([valid_img, valid_y, valid_input_len, valid_label_len], valid_output),\n",
    "                epochs=4, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 256, 64, 1)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict([valid_img, valid_y, valid_input_len, valid_label_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import  newaxis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = preds[:, :, newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_length=np.ones(preds.shape[0])\n",
    "input_length.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_decode(args):\n",
    "     y_pred, input_length =args\n",
    "     seq_len = tf.squeeze(input_length,axis=1)\n",
    "\n",
    "     return K.ctc_decode(y_pred=y_pred, input_length=seq_len, greedy=True, beam_width=100, top_paths=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded, _ = K.ctc_decode(preds, input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctc_decode = K.ctc_decode(preds,input_length)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded=K.get_value(ctc_decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = []\n",
    "for i in range(valid_size):    \n",
    "    prediction.append(num_to_label(decoded[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct characters predicted : 60.00%\n",
      "Correct words predicted      : 55.00%\n"
     ]
    }
   ],
   "source": [
    "y_true = validation.loc[0:valid_size, 'IDENTITY']\n",
    "correct_char = 0\n",
    "total_char = 0\n",
    "correct = 0\n",
    "\n",
    "for i in range(valid_size):\n",
    "    pr = prediction[i]\n",
    "    tr = y_true[i]\n",
    "    total_char += len(tr)\n",
    "    \n",
    "    for j in range(min(len(tr), len(pr))):\n",
    "        if tr[j] == pr[j]:\n",
    "            correct_char += 1\n",
    "            \n",
    "    if pr == tr :\n",
    "        correct += 1 \n",
    "    \n",
    "print('Correct characters predicted : %.2f%%' %(correct_char*100/total_char ))\n",
    "print('Correct words predicted      : %.2f%%' %(correct*100/valid_size ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
